{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 3 : Textual analysis of movie reviews\n",
    "\n",
    "** Due Date: November 17, 2016 5:59PM**\n",
    "\n",
    "*------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.conversational-technologies.com/nldemos/nlWordle.GIF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    member 1\n",
    "    \n",
    "    member 2\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desired outcome of the case study.**\n",
    "* In this case study we will look at movie reviews from the v2.0 polarity dataset comes from\n",
    "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
    "    * It contains written reviews of movies divided into positive and negative reviews.\n",
    "* As in Case Study 2 idea is to *analyze* the data set, make *conjectures*, support or refute those conjectures with *data*, and *tell a story* about the data!\n",
    "    \n",
    "**Required Readings:** \n",
    "* This case study will be based upon the scikit-learn Python library\n",
    "* We will build upon the turtorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "**Case study assumptions:**\n",
    "* You have access to a python installation\n",
    "\n",
    "**Required Python libraries:**\n",
    "* Numpy (www.numpy.org) (should already be installed from Case Study 2)\n",
    "* Matplotlib (matplotlib.org) (should already be installed from Case Study 2)\n",
    "* Scikit-learn (scikit-learn.org) (avaiable from Enthought Canopy)\n",
    "* You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (20 points): Complete Exercise 2: Sentiment Analysis on movie reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assuming that you have downloaded the scikit-learn source code:\n",
    "    * The data cane be downloaded using doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
    "    * A skeleton for the solution can be found in doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py\n",
    "    * A completed solution can be found in doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
    "* **It is ok to use the solution provided in the scikit-learn distribution as a starting place for your work.**\n",
    "\n",
    "### Modify the solution to Exercise 2 so that it can run in this iPython notebook\n",
    "* This will likely involved moving around data files and/or small modifications to the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libs\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "dataset = load_files('./text_analytics/data/movie_reviews/txt_sentoken/', shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and test data sets\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, \n",
    "                                                          test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building pipeline which is used to process the data in order\n",
    "pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)), # vectorization\n",
    "        ('clf', LinearSVC(C=1000)), # linear support vector classifier\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up parameters for voctorization process and use grid_search to fit the training data\n",
    "parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    }\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.82; std - 0.01\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.84; std - 0.00\n"
     ]
    }
   ],
   "source": [
    "# print out model score\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.88      0.86      0.87       252\n",
      "        pos       0.86      0.88      0.87       248\n",
      "\n",
      "avg / total       0.87      0.87      0.87       500\n",
      "\n",
      "[[217  35]\n",
      " [ 31 217]]\n"
     ]
    }
   ],
   "source": [
    "# predict the test data set and print out the result with best cross validation model\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "print(metrics.classification_report(y_test, y_predicted, \n",
    "                                    target_names=dataset.target_names)) # classification result\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted) # confusion matrix\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points): Explore the scikit-learn TfidVectorizer class\n",
    "\n",
    "**Read the documentation for the TfidVectorizer class at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.** \n",
    "* Define the term frequencyâ€“inverse document frequency (TF-IDF) statistic (http://en.wikipedia.org/wiki/Tf%E2%80%93idf will likely help).\n",
    "* Run the TfidVectorizer class on the training data above (docs_train).\n",
    "* Explore the min_df and max_df parameters of TfidVectorizer.  What do they mean? How do they change the features you get?\n",
    "* Explore the ngram_range parameter of TfidVectorizer.  What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current min_df is 0.050000 and max_df is 0.900000. The number of nonzero elements in matrix are 272183\n",
      "Current min_df is 0.100000 and max_df is 0.900000. The number of nonzero elements in matrix are 210586\n",
      "Current min_df is 0.150000 and max_df is 0.900000. The number of nonzero elements in matrix are 176235\n",
      "Current min_df is 0.200000 and max_df is 0.900000. The number of nonzero elements in matrix are 151707\n",
      "Current min_df is 0.250000 and max_df is 0.900000. The number of nonzero elements in matrix are 133792\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.05, 0.3, 0.05):\n",
    "    vector = TfidfVectorizer(min_df=i, max_df=0.9)\n",
    "    matrix = vector.fit_transform(docs_train)\n",
    "    print ('Current min_df is %f and max_df is %f. The number of nonzero elements in matrix are %d' % \n",
    "           (i, 0.9, matrix.nnz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current min_df is 0.100000 and max_df is 0.950000. The number of nonzero elements in matrix are 213400\n",
      "Current min_df is 0.100000 and max_df is 0.900000. The number of nonzero elements in matrix are 210586\n",
      "Current min_df is 0.100000 and max_df is 0.850000. The number of nonzero elements in matrix are 198676\n",
      "Current min_df is 0.100000 and max_df is 0.800000. The number of nonzero elements in matrix are 191232\n",
      "Current min_df is 0.100000 and max_df is 0.750000. The number of nonzero elements in matrix are 185535\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.95, 0.7, -0.05):\n",
    "    vector = TfidfVectorizer(min_df=0.1, max_df=i)\n",
    "    matrix = vector.fit_transform(docs_train)\n",
    "    print ('Current min_df is %f and max_df is %f. The number of nonzero elements in matrix are %d' % \n",
    "           (0.1, i, matrix.nnz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current ngram range is (1,1). The number of nonzero elements in matrix are 210586\n",
      "Current ngram range is (1,2). The number of nonzero elements in matrix are 279160\n",
      "Current ngram range is (1,3). The number of nonzero elements in matrix are 283401\n"
     ]
    }
   ],
   "source": [
    "ngram_candidate = [(1,1), (1,2), (1,3)]\n",
    "for ngram in ngram_candidate:\n",
    "    vector = TfidfVectorizer(min_df=0.1, max_df=0.9, ngram_range=ngram)\n",
    "    matrix = vector.fit_transform(docs_train)\n",
    "    print ('Current ngram range is (%d,%d). The number of nonzero elements in matrix are %d' % \n",
    "           (ngram[0], ngram[1], matrix.nnz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "## Problem 3 (20 points): Machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based upon Problem 2 pick some parameters for TfidfVectorizer\n",
    "    * \"fit\" your TfidfVectorizer using docs_train\n",
    "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
    "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
    "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
    "* Examine two classifiers provided by scikit-learn \n",
    "    * LinearSVC\n",
    "    * KNeighborsClassifier\n",
    "    * Try a number of different parameter settings for each and judge your performance using a confusion matrix (see Problem 1 for an example).\n",
    "* Does one classifier, or one set of parameters work better?\n",
    "    * Why do you think it might be working better?\n",
    "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
    "    * Can you conjecture on why the classifier made a mistake for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare the Xtrain and Xtest\n",
    "vector = TfidfVectorizer(min_df=3, max_df=0.95, ngram_range=(1,2))\n",
    "vector.fit(docs_train)\n",
    "\n",
    "Xtrain = vector.transform(docs_train)\n",
    "Xtest  = vector.transform(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parameters of LinearSVC are C=1000\n",
      "Final confusion matrix is:\n",
      "[[217  35]\n",
      " [ 31 217]]\n",
      "\n",
      "Current parameters of LinearSVC are C=900\n",
      "Final confusion matrix is:\n",
      "[[217  35]\n",
      " [ 31 217]]\n",
      "\n",
      "Current parameters of LinearSVC are C=800\n",
      "Final confusion matrix is:\n",
      "[[217  35]\n",
      " [ 31 217]]\n",
      "\n",
      "Current parameters of LinearSVC are C=700\n",
      "Final confusion matrix is:\n",
      "[[217  35]\n",
      " [ 31 217]]\n",
      "\n",
      "Current parameters of LinearSVC are C=600\n",
      "Final confusion matrix is:\n",
      "[[217  35]\n",
      " [ 31 217]]\n",
      "\n",
      "Current parameters of LinearSVC are C=500\n",
      "Final confusion matrix is:\n",
      "[[217  35]\n",
      " [ 31 217]]\n",
      "\n",
      "Current parameters of LinearSVC are C=400\n",
      "Final confusion matrix is:\n",
      "[[217  35]\n",
      " [ 31 217]]\n",
      "\n",
      "Current parameters of LinearSVC are C=300\n",
      "Final confusion matrix is:\n",
      "[[217  35]\n",
      " [ 31 217]]\n",
      "\n",
      "Current parameters of LinearSVC are C=200\n",
      "Final confusion matrix is:\n",
      "[[217  35]\n",
      " [ 31 217]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine LinearSVC\n",
    "svc_C = [1000, 900, 800, 700, 600, 500, 400, 300, 200]\n",
    "\n",
    "for param in svc_C:\n",
    "    svc = LinearSVC(C=param)\n",
    "    svc.fit(Xtrain, y_train)\n",
    "    y_predicted = svc.predict(Xtest)\n",
    "    \n",
    "    print ('Current parameters of LinearSVC are C=%d' % param)\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted) # confusion matrix\n",
    "    print ('Final confusion matrix is:')\n",
    "    print (cm)\n",
    "    print ('\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parameters of KNN classifier are n_neighbors=1\n",
      "Final confusion matrix is:\n",
      "[[162  90]\n",
      " [ 74 174]]\n",
      "\n",
      "Current parameters of KNN classifier are n_neighbors=2\n",
      "Final confusion matrix is:\n",
      "[[193  59]\n",
      " [124 124]]\n",
      "\n",
      "Current parameters of KNN classifier are n_neighbors=3\n",
      "Final confusion matrix is:\n",
      "[[150 102]\n",
      " [ 65 183]]\n",
      "\n",
      "Current parameters of KNN classifier are n_neighbors=4\n",
      "Final confusion matrix is:\n",
      "[[181  71]\n",
      " [ 93 155]]\n",
      "\n",
      "Current parameters of KNN classifier are n_neighbors=5\n",
      "Final confusion matrix is:\n",
      "[[135 117]\n",
      " [ 50 198]]\n",
      "\n",
      "Current parameters of KNN classifier are n_neighbors=6\n",
      "Final confusion matrix is:\n",
      "[[161  91]\n",
      " [ 67 181]]\n",
      "\n",
      "Current parameters of KNN classifier are n_neighbors=7\n",
      "Final confusion matrix is:\n",
      "[[146 106]\n",
      " [ 45 203]]\n",
      "\n",
      "Current parameters of KNN classifier are n_neighbors=8\n",
      "Final confusion matrix is:\n",
      "[[163  89]\n",
      " [ 61 187]]\n",
      "\n",
      "Current parameters of KNN classifier are n_neighbors=9\n",
      "Final confusion matrix is:\n",
      "[[142 110]\n",
      " [ 45 203]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# examine LinearSVC\n",
    "KNN_n_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "for param in KNN_n_neighbors:\n",
    "    KNN = KNeighborsClassifier(n_neighbors=param)\n",
    "    KNN.fit(Xtrain, y_train)\n",
    "    y_predicted = KNN.predict(Xtest)\n",
    "    \n",
    "    print ('Current parameters of KNN classifier are n_neighbors=%d' % param)\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted) # confusion matrix\n",
    "    print ('Final confusion matrix is:')\n",
    "    print (cm)\n",
    "    print ('\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Start Evaluation of LinearSVC =======\n",
      "0 params - {'vect__ngram_range': (1, 2), 'clf__C': 1000, 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "1 params - {'vect__ngram_range': (1, 3), 'clf__C': 1000, 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "2 params - {'vect__ngram_range': (1, 2), 'clf__C': 1000, 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "3 params - {'vect__ngram_range': (1, 3), 'clf__C': 1000, 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "4 params - {'vect__ngram_range': (1, 2), 'clf__C': 1000, 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "5 params - {'vect__ngram_range': (1, 3), 'clf__C': 1000, 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "6 params - {'vect__ngram_range': (1, 2), 'clf__C': 1000, 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "7 params - {'vect__ngram_range': (1, 3), 'clf__C': 1000, 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "8 params - {'vect__ngram_range': (1, 2), 'clf__C': 1000, 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.84; std - 0.00\n",
      "9 params - {'vect__ngram_range': (1, 3), 'clf__C': 1000, 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "10 params - {'vect__ngram_range': (1, 2), 'clf__C': 1000, 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "11 params - {'vect__ngram_range': (1, 3), 'clf__C': 1000, 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "12 params - {'vect__ngram_range': (1, 2), 'clf__C': 1000, 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "13 params - {'vect__ngram_range': (1, 3), 'clf__C': 1000, 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.84; std - 0.01\n",
      "14 params - {'vect__ngram_range': (1, 2), 'clf__C': 1000, 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "15 params - {'vect__ngram_range': (1, 3), 'clf__C': 1000, 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.84; std - 0.01\n",
      "16 params - {'vect__ngram_range': (1, 2), 'clf__C': 1000, 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "17 params - {'vect__ngram_range': (1, 3), 'clf__C': 1000, 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "18 params - {'vect__ngram_range': (1, 2), 'clf__C': 800, 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "19 params - {'vect__ngram_range': (1, 3), 'clf__C': 800, 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "20 params - {'vect__ngram_range': (1, 2), 'clf__C': 800, 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "21 params - {'vect__ngram_range': (1, 3), 'clf__C': 800, 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "22 params - {'vect__ngram_range': (1, 2), 'clf__C': 800, 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "23 params - {'vect__ngram_range': (1, 3), 'clf__C': 800, 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "24 params - {'vect__ngram_range': (1, 2), 'clf__C': 800, 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "25 params - {'vect__ngram_range': (1, 3), 'clf__C': 800, 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "26 params - {'vect__ngram_range': (1, 2), 'clf__C': 800, 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.84; std - 0.00\n",
      "27 params - {'vect__ngram_range': (1, 3), 'clf__C': 800, 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "28 params - {'vect__ngram_range': (1, 2), 'clf__C': 800, 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "29 params - {'vect__ngram_range': (1, 3), 'clf__C': 800, 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "30 params - {'vect__ngram_range': (1, 2), 'clf__C': 800, 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "31 params - {'vect__ngram_range': (1, 3), 'clf__C': 800, 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.84; std - 0.01\n",
      "32 params - {'vect__ngram_range': (1, 2), 'clf__C': 800, 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "33 params - {'vect__ngram_range': (1, 3), 'clf__C': 800, 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.84; std - 0.01\n",
      "34 params - {'vect__ngram_range': (1, 2), 'clf__C': 800, 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "35 params - {'vect__ngram_range': (1, 3), 'clf__C': 800, 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "36 params - {'vect__ngram_range': (1, 2), 'clf__C': 600, 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "37 params - {'vect__ngram_range': (1, 3), 'clf__C': 600, 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "38 params - {'vect__ngram_range': (1, 2), 'clf__C': 600, 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "39 params - {'vect__ngram_range': (1, 3), 'clf__C': 600, 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "40 params - {'vect__ngram_range': (1, 2), 'clf__C': 600, 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "41 params - {'vect__ngram_range': (1, 3), 'clf__C': 600, 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "42 params - {'vect__ngram_range': (1, 2), 'clf__C': 600, 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "43 params - {'vect__ngram_range': (1, 3), 'clf__C': 600, 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "44 params - {'vect__ngram_range': (1, 2), 'clf__C': 600, 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.84; std - 0.00\n",
      "45 params - {'vect__ngram_range': (1, 3), 'clf__C': 600, 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "46 params - {'vect__ngram_range': (1, 2), 'clf__C': 600, 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "47 params - {'vect__ngram_range': (1, 3), 'clf__C': 600, 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "48 params - {'vect__ngram_range': (1, 2), 'clf__C': 600, 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "49 params - {'vect__ngram_range': (1, 3), 'clf__C': 600, 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.84; std - 0.01\n",
      "50 params - {'vect__ngram_range': (1, 2), 'clf__C': 600, 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "51 params - {'vect__ngram_range': (1, 3), 'clf__C': 600, 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.84; std - 0.01\n",
      "52 params - {'vect__ngram_range': (1, 2), 'clf__C': 600, 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "53 params - {'vect__ngram_range': (1, 3), 'clf__C': 600, 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.87      0.85      0.86       252\n",
      "        pos       0.85      0.87      0.86       248\n",
      "\n",
      "avg / total       0.86      0.86      0.86       500\n",
      "\n",
      "[[215  37]\n",
      " [ 33 215]]\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC\n",
    "print ('======= Start Evaluation of LinearSVC =======')\n",
    "pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer()), # vectorization\n",
    "        ('clf', LinearSVC()), # linear support vector classifier\n",
    "    ])\n",
    "\n",
    "parameters = {\n",
    "        'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "        'vect__min_df': [2, 3, 4],\n",
    "        'vect__max_df': [0.98, 0.95, 0.92],\n",
    "        'clf__C': [1000, 800, 600],\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)\n",
    "\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "print(metrics.classification_report(y_test, y_predicted, \n",
    "                                    target_names=dataset.target_names)) # classification result\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted) # confusion matrix\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Start Evaluation of KNN =======\n",
      "0 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.64; std - 0.02\n",
      "1 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.64; std - 0.01\n",
      "2 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.65; std - 0.02\n",
      "3 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.65; std - 0.02\n",
      "4 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.65; std - 0.02\n",
      "5 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.66; std - 0.02\n",
      "6 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.64; std - 0.02\n",
      "7 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.64; std - 0.01\n",
      "8 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.65; std - 0.01\n",
      "9 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.65; std - 0.02\n",
      "10 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.65; std - 0.01\n",
      "11 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.66; std - 0.02\n",
      "12 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.64; std - 0.02\n",
      "13 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.64; std - 0.01\n",
      "14 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.65; std - 0.02\n",
      "15 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.65; std - 0.02\n",
      "16 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.65; std - 0.01\n",
      "17 params - {'clf__n_neighbors': 2, 'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.66; std - 0.01\n",
      "18 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.65; std - 0.01\n",
      "19 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.66; std - 0.00\n",
      "20 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.67; std - 0.01\n",
      "21 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.67; std - 0.01\n",
      "22 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.67; std - 0.00\n",
      "23 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.67; std - 0.01\n",
      "24 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.65; std - 0.01\n",
      "25 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.66; std - 0.01\n",
      "26 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.67; std - 0.01\n",
      "27 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.66; std - 0.00\n",
      "28 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.67; std - 0.01\n",
      "29 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.67; std - 0.00\n",
      "30 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.66; std - 0.02\n",
      "31 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.65; std - 0.01\n",
      "32 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.67; std - 0.00\n",
      "33 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.67; std - 0.01\n",
      "34 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.67; std - 0.01\n",
      "35 params - {'clf__n_neighbors': 4, 'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.67; std - 0.01\n",
      "36 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.67; std - 0.01\n",
      "37 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.98}; mean - 0.69; std - 0.01\n",
      "38 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.67; std - 0.01\n",
      "39 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.98}; mean - 0.67; std - 0.01\n",
      "40 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.68; std - 0.01\n",
      "41 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.98}; mean - 0.69; std - 0.01\n",
      "42 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.67; std - 0.01\n",
      "43 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.95}; mean - 0.69; std - 0.01\n",
      "44 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.68; std - 0.01\n",
      "45 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.95}; mean - 0.67; std - 0.01\n",
      "46 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.68; std - 0.00\n",
      "47 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.95}; mean - 0.69; std - 0.01\n",
      "48 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.67; std - 0.01\n",
      "49 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.92}; mean - 0.69; std - 0.01\n",
      "50 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.68; std - 0.01\n",
      "51 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.92}; mean - 0.68; std - 0.01\n",
      "52 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.68; std - 0.01\n",
      "53 params - {'clf__n_neighbors': 6, 'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.92}; mean - 0.69; std - 0.00\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.71      0.67      0.69       252\n",
      "        pos       0.68      0.73      0.70       248\n",
      "\n",
      "avg / total       0.70      0.70      0.70       500\n",
      "\n",
      "[[169  83]\n",
      " [ 68 180]]\n"
     ]
    }
   ],
   "source": [
    "# K nearst neighbor\n",
    "print ('======= Start Evaluation of KNN =======')\n",
    "pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer()), # vectorization\n",
    "        ('clf', KNeighborsClassifier()), # KNN classifier\n",
    "    ])\n",
    "\n",
    "parameters = {\n",
    "        'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "        'vect__min_df': [2, 3, 4],\n",
    "        'vect__max_df': [0.98, 0.95, 0.92],\n",
    "        'clf__n_neighbors': [2, 4, 6],\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)\n",
    "\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "print(metrics.classification_report(y_test, y_predicted, \n",
    "                                    target_names=dataset.target_names)) # classification result\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted) # confusion matrix\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Start Evaluation of Random Forest =======\n",
      "0 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.98, 'clf__n_estimators': 4}; mean - 0.59; std - 0.02\n",
      "1 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.98, 'clf__n_estimators': 4}; mean - 0.59; std - 0.02\n",
      "2 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.98, 'clf__n_estimators': 4}; mean - 0.61; std - 0.01\n",
      "3 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.98, 'clf__n_estimators': 4}; mean - 0.61; std - 0.01\n",
      "4 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.98, 'clf__n_estimators': 4}; mean - 0.59; std - 0.03\n",
      "5 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.98, 'clf__n_estimators': 4}; mean - 0.61; std - 0.00\n",
      "6 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.95, 'clf__n_estimators': 4}; mean - 0.60; std - 0.01\n",
      "7 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.95, 'clf__n_estimators': 4}; mean - 0.59; std - 0.02\n",
      "8 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.95, 'clf__n_estimators': 4}; mean - 0.60; std - 0.01\n",
      "9 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.95, 'clf__n_estimators': 4}; mean - 0.59; std - 0.01\n",
      "10 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.95, 'clf__n_estimators': 4}; mean - 0.61; std - 0.00\n",
      "11 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.95, 'clf__n_estimators': 4}; mean - 0.61; std - 0.01\n",
      "12 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.92, 'clf__n_estimators': 4}; mean - 0.59; std - 0.02\n",
      "13 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.92, 'clf__n_estimators': 4}; mean - 0.60; std - 0.02\n",
      "14 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.92, 'clf__n_estimators': 4}; mean - 0.61; std - 0.03\n",
      "15 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.92, 'clf__n_estimators': 4}; mean - 0.61; std - 0.02\n",
      "16 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.92, 'clf__n_estimators': 4}; mean - 0.61; std - 0.01\n",
      "17 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.92, 'clf__n_estimators': 4}; mean - 0.60; std - 0.02\n",
      "18 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.98, 'clf__n_estimators': 8}; mean - 0.63; std - 0.02\n",
      "19 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.98, 'clf__n_estimators': 8}; mean - 0.65; std - 0.01\n",
      "20 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.98, 'clf__n_estimators': 8}; mean - 0.65; std - 0.01\n",
      "21 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.98, 'clf__n_estimators': 8}; mean - 0.64; std - 0.01\n",
      "22 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.98, 'clf__n_estimators': 8}; mean - 0.62; std - 0.03\n",
      "23 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.98, 'clf__n_estimators': 8}; mean - 0.63; std - 0.01\n",
      "24 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.95, 'clf__n_estimators': 8}; mean - 0.61; std - 0.01\n",
      "25 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.95, 'clf__n_estimators': 8}; mean - 0.64; std - 0.01\n",
      "26 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.95, 'clf__n_estimators': 8}; mean - 0.65; std - 0.02\n",
      "27 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.95, 'clf__n_estimators': 8}; mean - 0.63; std - 0.03\n",
      "28 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.95, 'clf__n_estimators': 8}; mean - 0.64; std - 0.01\n",
      "29 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.95, 'clf__n_estimators': 8}; mean - 0.63; std - 0.01\n",
      "30 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.92, 'clf__n_estimators': 8}; mean - 0.62; std - 0.01\n",
      "31 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.92, 'clf__n_estimators': 8}; mean - 0.64; std - 0.02\n",
      "32 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.92, 'clf__n_estimators': 8}; mean - 0.62; std - 0.01\n",
      "33 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.92, 'clf__n_estimators': 8}; mean - 0.63; std - 0.03\n",
      "34 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.92, 'clf__n_estimators': 8}; mean - 0.64; std - 0.04\n",
      "35 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.92, 'clf__n_estimators': 8}; mean - 0.64; std - 0.04\n",
      "36 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.98, 'clf__n_estimators': 12}; mean - 0.65; std - 0.02\n",
      "37 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.98, 'clf__n_estimators': 12}; mean - 0.65; std - 0.01\n",
      "38 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.98, 'clf__n_estimators': 12}; mean - 0.66; std - 0.01\n",
      "39 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.98, 'clf__n_estimators': 12}; mean - 0.66; std - 0.00\n",
      "40 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.98, 'clf__n_estimators': 12}; mean - 0.65; std - 0.02\n",
      "41 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.98, 'clf__n_estimators': 12}; mean - 0.67; std - 0.02\n",
      "42 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.95, 'clf__n_estimators': 12}; mean - 0.66; std - 0.01\n",
      "43 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.95, 'clf__n_estimators': 12}; mean - 0.66; std - 0.05\n",
      "44 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.95, 'clf__n_estimators': 12}; mean - 0.66; std - 0.01\n",
      "45 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.95, 'clf__n_estimators': 12}; mean - 0.63; std - 0.01\n",
      "46 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.95, 'clf__n_estimators': 12}; mean - 0.68; std - 0.03\n",
      "47 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.95, 'clf__n_estimators': 12}; mean - 0.65; std - 0.02\n",
      "48 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect__max_df': 0.92, 'clf__n_estimators': 12}; mean - 0.67; std - 0.01\n",
      "49 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'vect__max_df': 0.92, 'clf__n_estimators': 12}; mean - 0.66; std - 0.00\n",
      "50 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'vect__max_df': 0.92, 'clf__n_estimators': 12}; mean - 0.66; std - 0.02\n",
      "51 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'vect__max_df': 0.92, 'clf__n_estimators': 12}; mean - 0.63; std - 0.01\n",
      "52 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'vect__max_df': 0.92, 'clf__n_estimators': 12}; mean - 0.66; std - 0.01\n",
      "53 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'vect__max_df': 0.92, 'clf__n_estimators': 12}; mean - 0.67; std - 0.02\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.64      0.85      0.73       252\n",
      "        pos       0.77      0.52      0.62       248\n",
      "\n",
      "avg / total       0.71      0.69      0.68       500\n",
      "\n",
      "[[214  38]\n",
      " [118 130]]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print ('======= Start Evaluation of Random Forest =======')\n",
    "pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer()), # vectorization\n",
    "        ('clf', RandomForestClassifier()), # random forest classifier\n",
    "    ])\n",
    "\n",
    "parameters = {\n",
    "        'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "        'vect__min_df': [2, 3, 4],\n",
    "        'vect__max_df': [0.98, 0.95, 0.92],\n",
    "        'clf__n_estimators': [4, 8, 12],\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)\n",
    "\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "print(metrics.classification_report(y_test, y_predicted, \n",
    "                                    target_names=dataset.target_names)) # classification result\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted) # confusion matrix\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Start Evaluation of Random Forest =======\n",
      "0 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'clf__alpha': 1e-05, 'vect__max_df': 0.98}; mean - 0.84; std - 0.00\n",
      "1 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'clf__alpha': 1e-05, 'vect__max_df': 0.98}; mean - 0.84; std - 0.00\n",
      "2 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'clf__alpha': 1e-05, 'vect__max_df': 0.98}; mean - 0.84; std - 0.00\n",
      "3 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'clf__alpha': 1e-05, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "4 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'clf__alpha': 1e-05, 'vect__max_df': 0.98}; mean - 0.84; std - 0.00\n",
      "5 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'clf__alpha': 1e-05, 'vect__max_df': 0.98}; mean - 0.85; std - 0.01\n",
      "6 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'clf__alpha': 1e-05, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "7 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'clf__alpha': 1e-05, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "8 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'clf__alpha': 1e-05, 'vect__max_df': 0.95}; mean - 0.83; std - 0.00\n",
      "9 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'clf__alpha': 1e-05, 'vect__max_df': 0.95}; mean - 0.85; std - 0.01\n",
      "10 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'clf__alpha': 1e-05, 'vect__max_df': 0.95}; mean - 0.84; std - 0.00\n",
      "11 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'clf__alpha': 1e-05, 'vect__max_df': 0.95}; mean - 0.85; std - 0.01\n",
      "12 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'clf__alpha': 1e-05, 'vect__max_df': 0.92}; mean - 0.83; std - 0.01\n",
      "13 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'clf__alpha': 1e-05, 'vect__max_df': 0.92}; mean - 0.83; std - 0.01\n",
      "14 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'clf__alpha': 1e-05, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "15 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'clf__alpha': 1e-05, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "16 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'clf__alpha': 1e-05, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "17 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'clf__alpha': 1e-05, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "18 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'clf__alpha': 0.0001, 'vect__max_df': 0.98}; mean - 0.83; std - 0.00\n",
      "19 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'clf__alpha': 0.0001, 'vect__max_df': 0.98}; mean - 0.83; std - 0.01\n",
      "20 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'clf__alpha': 0.0001, 'vect__max_df': 0.98}; mean - 0.84; std - 0.00\n",
      "21 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'clf__alpha': 0.0001, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "22 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'clf__alpha': 0.0001, 'vect__max_df': 0.98}; mean - 0.85; std - 0.00\n",
      "23 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'clf__alpha': 0.0001, 'vect__max_df': 0.98}; mean - 0.85; std - 0.01\n",
      "24 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'clf__alpha': 0.0001, 'vect__max_df': 0.95}; mean - 0.83; std - 0.00\n",
      "25 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'clf__alpha': 0.0001, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "26 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'clf__alpha': 0.0001, 'vect__max_df': 0.95}; mean - 0.84; std - 0.00\n",
      "27 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'clf__alpha': 0.0001, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "28 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'clf__alpha': 0.0001, 'vect__max_df': 0.95}; mean - 0.85; std - 0.00\n",
      "29 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'clf__alpha': 0.0001, 'vect__max_df': 0.95}; mean - 0.84; std - 0.00\n",
      "30 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'clf__alpha': 0.0001, 'vect__max_df': 0.92}; mean - 0.84; std - 0.01\n",
      "31 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'clf__alpha': 0.0001, 'vect__max_df': 0.92}; mean - 0.83; std - 0.01\n",
      "32 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'clf__alpha': 0.0001, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "33 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'clf__alpha': 0.0001, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "34 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'clf__alpha': 0.0001, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "35 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'clf__alpha': 0.0001, 'vect__max_df': 0.92}; mean - 0.85; std - 0.01\n",
      "36 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'clf__alpha': 0.001, 'vect__max_df': 0.98}; mean - 0.83; std - 0.01\n",
      "37 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'clf__alpha': 0.001, 'vect__max_df': 0.98}; mean - 0.83; std - 0.00\n",
      "38 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'clf__alpha': 0.001, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "39 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'clf__alpha': 0.001, 'vect__max_df': 0.98}; mean - 0.84; std - 0.00\n",
      "40 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'clf__alpha': 0.001, 'vect__max_df': 0.98}; mean - 0.85; std - 0.00\n",
      "41 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'clf__alpha': 0.001, 'vect__max_df': 0.98}; mean - 0.84; std - 0.01\n",
      "42 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'clf__alpha': 0.001, 'vect__max_df': 0.95}; mean - 0.84; std - 0.00\n",
      "43 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'clf__alpha': 0.001, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "44 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'clf__alpha': 0.001, 'vect__max_df': 0.95}; mean - 0.83; std - 0.01\n",
      "45 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'clf__alpha': 0.001, 'vect__max_df': 0.95}; mean - 0.85; std - 0.01\n",
      "46 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'clf__alpha': 0.001, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "47 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'clf__alpha': 0.001, 'vect__max_df': 0.95}; mean - 0.84; std - 0.01\n",
      "48 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'clf__alpha': 0.001, 'vect__max_df': 0.92}; mean - 0.83; std - 0.01\n",
      "49 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 2, 'clf__alpha': 0.001, 'vect__max_df': 0.92}; mean - 0.84; std - 0.01\n",
      "50 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 3, 'clf__alpha': 0.001, 'vect__max_df': 0.92}; mean - 0.84; std - 0.00\n",
      "51 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 3, 'clf__alpha': 0.001, 'vect__max_df': 0.92}; mean - 0.85; std - 0.00\n",
      "52 params - {'vect__ngram_range': (1, 2), 'vect__min_df': 4, 'clf__alpha': 0.001, 'vect__max_df': 0.92}; mean - 0.85; std - 0.00\n",
      "53 params - {'vect__ngram_range': (1, 3), 'vect__min_df': 4, 'clf__alpha': 0.001, 'vect__max_df': 0.92}; mean - 0.85; std - 0.00\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.88      0.88       252\n",
      "        pos       0.88      0.89      0.88       248\n",
      "\n",
      "avg / total       0.88      0.88      0.88       500\n",
      "\n",
      "[[221  31]\n",
      " [ 27 221]]\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "print ('======= Start Evaluation of Neural Network =======')\n",
    "pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer()), # vectorization\n",
    "        ('clf', MLPClassifier()), # neural network\n",
    "    ])\n",
    "\n",
    "parameters = {\n",
    "        'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "        'vect__min_df': [2, 3, 4],\n",
    "        'vect__max_df': [0.98, 0.95, 0.92],\n",
    "        'clf__alpha': [1e-5, 1e-4, 1e-3],\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)\n",
    "\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "print(metrics.classification_report(y_test, y_predicted, \n",
    "                                    target_names=dataset.target_names)) # classification result\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted) # confusion matrix\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "## Problem 4 (20 points): Open Ended Question:  Finding the right plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you find a two dimensional plot in which the positive and negative reviews are separated?\n",
    "    * This problem is hard since you will likely have thousands of features for review, and you will need to transform these thousands of features into just two numbers (so that you can make a 2D plot).\n",
    "* Note, I was not able to find such a plot myself!\n",
    "    * So, this problem is about **trying** but perhaps **not necessarily succeeding**!\n",
    "* I tried two things, neither of which worked very well.\n",
    "    * I first plotted the length of the review versus the number of features we compute that are in that review\n",
    "    * Second I used Principle Component Analysis on a subset of the features.\n",
    "* Can you do better than I did!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: communicate the results (20 points)\n",
    "\n",
    "(1) (5 points) What data you collected?\n",
    "\n",
    "(2) (5 points) Why this topic is interesting or important to you? (Motivations)\n",
    "\n",
    "(3) (5 points) How did you analyse the data?\n",
    "\n",
    "(4) (5 points) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slides (for 10 minutes of presentation) (20 points)\n",
    "\n",
    "\n",
    "1. (5 points) Motivation about the data collection, why the topic is interesting to you. \n",
    "\n",
    "2. (10 points) Communicating Results (figure/table)\n",
    "\n",
    "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What is the relationship between this topic and Business Intelligence?\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    "    * What conjectures did you make and how did you support or disprove them using data?\n",
    "    * Did you find anything suprising in the data?\n",
    "    * What business decision do you think this data could help answer?  Why?\n",
    "\n",
    "   (please include figures or tables in the report, **but no source code**)\n",
    "\n",
    "*Please compress all the files into a single zipped file.*\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Send an email to rcpaffenroth@wpi.edu and wliu3@wpi.edu with the subject: \"[DS501] Case study 3-TEAM NUMBER ???\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
